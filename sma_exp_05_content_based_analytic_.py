# -*- coding: utf-8 -*-
"""SMA EXP 05 Content Based analytic .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZVKmLpWflQKVpwIxpwwTYDLiBfhxbop
"""

import pandas as pd

!pip install snscrape

import snscrape.modules.twitter as sntwitter

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

# Commented out IPython magic to ensure Python compatibility.

import string
import re
import textblob
from textblob import TextBlob

from wordcloud import WordCloud, STOPWORDS

from wordcloud import ImageColorGenerator

import warnings
# %matplotlib inline

import os

# Using OS library to call CLI commands in Python
os.system("snscrape --jsonl --max-results 5000 --since 2023-01-31 twitter-search 'Budget 2023 until:2023-02-03' > text-query-tweets.json")

import pandas as pd

# Reads the json generated from the CLI commands above and creates a pandas dataframe
tweets_df = pd.read_json('text-query-tweets.json', lines=True)

tweets_df.head()

tweets_df.to_csv()

df = pd.read_csv("./Alan_walker_Hello_world.csv")

df.head(5)

print(df.shape)

df.info()
df.pubdate.value_counts()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

#Heat Map for missing values
plt.figure(figsize=(17, 5))
sns.heatmap(df.isnull(), cbar=True, yticklabels=False)
plt.xlabel("Column_Name", size=14, weight="bold")
plt.title("Places of missing values in column",fontweight="bold",size=17)
plt.show()

import plotly.graph_objects as go
Top_Title_Of_tweet= df['title'].value_counts().head(10)

print (Top_Title_Of_tweet)

import nltk

stop=nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
df['description'].apply(lambda x: [item for item in x if item not in stop])
df.shape
df.head()

!pip install tweet-preprocessor

#Remove unnecessary characters
punct =['%','/',':','\\','&amp;','&',';', '?']

def remove_punctuations(text):
    for punctuation in punct:
        text = text.replace(punctuation, '')
    return text

df=df.drop_duplicates('description')

from nltk.corpus import stopwords
stop = stopwords.words('english')
df['description'].apply(lambda x: [item for item in x if item not in stop])
df.shape

df['description'] = df['description'].apply(lambda x: remove_punctuations(x))

#Drop tweets which have empty text field
df['description'].replace(' ', np.nan, inplace=True)
df.dropna(subset=['description'], inplace=True)
len(df)

df = df.reset_index(drop=True)
df.sample(5)

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer



# Commented out IPython magic to ensure Python compatibility.
sns.set_style('whitegrid')
# %matplotlib inline
stop=stop +['hello','good','http','love','happy','Thank','heart','me','mood','video','india']
def plot_20_most_common_words(count_data, count_vectorizer):
    import matplotlib.pyplot as plt
    words = count_vectorizer.get_feature_names_out()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]

    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:20]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words))

    plt.figure(2, figsize=(40, 40))
    plt.subplot(title='20 most common words')
    sns.set_context("notebook", font_scale=4, rc={"lines.linewidth": 2.5})

    sns.barplot(x=x_pos, y=counts, palette='husl')
    #sns.barplot(x_pos, counts, palette='husl')
    plt.xticks(x_pos, words, rotation=90)
    plt.xlabel('words')
    plt.ylabel('counts')
    plt.show()
 # Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words=stop)
# Fit and transform the processed titles
count_data = count_vectorizer.fit_transform(df['description'])
# Visualise the 20 most common words
plot_20_most_common_words(count_data, count_vectorizer)
plt.savefig('saved_figure.png')

import cufflinks as cf
cf.go_offline()
cf.set_config_file(offline=False, world_readable=True)

def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 4), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_bigram(df['description'], 8)
mydict={}
for word, freq in common_words:
    bigram_df  = pd.DataFrame(common_words, columns = ['ngram' , 'count'])
bigram_df.groupby('ngram').sum()['count'].sort_values(ascending=False).sort_values().plot.barh(title='Top 8 bigrams ', color='orange', width=.4, figsize=(12, 8), stacked = True)

#iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 10 bigrams ')

!pip install textblob

from textblob import TextBlob

def get_subjectivity(text):
    return TextBlob(text).sentiment.subjectivity

def get_polarity(text):
    return TextBlob(text).sentiment.polarity

df['subjectivity'] = df['description'].apply(get_subjectivity)
df['polarity'] = df['description'].apply(get_polarity)
df.head()

# Obtain polarity scores generated by TextBlob
df['textblob_score'] = df['description'].apply(lambda x: TextBlob(x).sentiment.polarity)

neutral_threshold = 0.05

# Convert polarity score into sentiment categories
df['textblob_sentiment'] = df['textblob_score'].apply(lambda c: 'Positive' if c >= neutral_threshold else ('Negative' if c <= -(neutral_threshold) else 'Neutral'))

textblob_df = df[['description', 'textblob_sentiment', 'likecount']]
textblob_df

# @title textblob_sentiment

from matplotlib import pyplot as plt
import seaborn as sns
textblob_df.groupby('textblob_sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

textblob_df['textblob_sentiment'].value_counts()

textblob_df['textblob_sentiment'].value_counts().plot.barh(title='Sentiment Analysis ', color='orange', width=.4, figsize=(10, 8), stacked = True)

df_positive=textblob_df[textblob_df['textblob_sentiment']=='Positive']

df_Very_positive=df_positive[df_positive['likecount']>0]

df_Very_positive.head()

df_negative=textblob_df[textblob_df['textblob_sentiment']=='Negative']

df_negative.head()

df_neutral=textblob_df[textblob_df['textblob_sentiment']=='Neutral']

df_neutral.head()

from wordcloud import WordCloud, STOPWORDS
from PIL import Image

#Creating the text variable
positive_tw = " ".join(t for t in df_Very_positive.description)

# Creating word_cloud with text as argument in .generate() method

word_cloud1 = WordCloud(collocations = False, background_color = 'white').generate(positive_tw)

# Display the generated Word Cloud

plt.imshow(word_cloud1, interpolation='bilinear')

plt.axis("off")

plt.show()

#Creating the text variable
negative_tw = " ".join(t for t in df_negative.description)

# Creating word_cloud with text as argument in .generate() method

word_cloud2 = WordCloud(collocations = False, background_color = 'white').generate(negative_tw)

# Display the generated Word Cloud

plt.imshow(word_cloud2, interpolation='bilinear')

plt.axis("off")

plt.show()

